{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as num\n",
    "import scipy as sci\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import sklearn as skl\n",
    "import xgboost\n",
    "import gensim\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a fraction of the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingAmount = 10000\n",
    "testAmount = 2500\n",
    "\n",
    "with open('review.json') as raw:\n",
    "    with open('training.json', 'w') as trainingSet: \n",
    "        for x in range(trainingAmount):\n",
    "            line = raw.readline()\n",
    "            trainingSet.write(line)\n",
    "    with open('test.json', 'w') as testSet: \n",
    "        for x in range(trainingAmount, trainingAmount+testAmount):\n",
    "            line = raw.readline()\n",
    "            testSet.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_json('training.json', lines=True)\n",
    "df_test = pd.read_json('test.json', lines=True)\n",
    "\n",
    "df_test = df_test.drop(\"review_id\", axis=1).drop(\"business_id\", axis=1).drop(\"user_id\", axis=1).drop(\"date\", axis=1)\n",
    "df_test = df_test.reindex(['text','stars','useful','funny','cool'], axis=1)\n",
    "\n",
    "df_train = df_train.drop(\"review_id\", axis=1).drop(\"business_id\", axis=1).drop(\"user_id\", axis=1).drop(\"date\", axis=1)\n",
    "df_train = df_train.reindex(['text','stars','useful','funny','cool'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the clean method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    from nltk.stem.snowball import SnowballStemmer\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    \n",
    "#     lemmatizer = WordNetLemmatizer()\n",
    "#     for word in text:\n",
    "#         word = lemmatizer.lemmatize(word)\n",
    "    \n",
    "     # Empty question\n",
    "    if type(text) != str:\n",
    "        return ''\n",
    "    \n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "    \n",
    "    text = re.sub(\"I\\'ll\", \"I will\", text)\n",
    "    text = re.sub(\"I\\'m\", \"I am\", text)\n",
    "\n",
    "    text = text.lower()\n",
    "    \n",
    "    text = re.sub(\" what\\'s \", \" what is \", text)\n",
    "    text = re.sub(\" that\\'s \", \" that is \", text)\n",
    "    \n",
    "    text = re.sub(\"\\'s\", \"\", text)\n",
    "    text = re.sub('\\n', ' ', text)\n",
    "    \n",
    "    \n",
    "    text = re.sub(\" whats \", \" what is \", text)\n",
    "    text = re.sub(\" thats \", \" that is \", text)\n",
    "    text = re.sub(\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(\"n\\'t\", ' not', text)\n",
    "\n",
    "    text = re.sub('[' + string.punctuation + ']', '', text)\n",
    "    \n",
    "    # remove comma between numbers, i.e. 15,000 -> 15000\n",
    "    text = re.sub('(?<=[0-9])\\,(?=[0-9])', \"\", text)\n",
    "    \n",
    "#     for sw in stopwords:\n",
    "#         text = re.sub(' ' + sw + ' ', ' ', text)\n",
    "    \n",
    "    tokenized_text = word_tokenize(text) \n",
    "    \n",
    "    tokenized_clean_text = [w for w in tokenized_text if not w in stopwords] \n",
    "    \n",
    "    \n",
    "#     stemmer = SnowballStemmer('english')\n",
    "#     textList = word_tokenize(text)\n",
    "#     text = ''\n",
    "#     for word in textList:\n",
    "#         text += stemmer.stem(word) + ' '\n",
    "    \n",
    "    \n",
    "    # Return a list of words\n",
    "    return tokenized_clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['length'] = df_train['text'].apply(len)\n",
    "df_test['length'] = df_test['text'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stars</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.00000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.716700</td>\n",
       "      <td>1.29820</td>\n",
       "      <td>0.458000</td>\n",
       "      <td>0.559000</td>\n",
       "      <td>581.699600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.471549</td>\n",
       "      <td>2.90264</td>\n",
       "      <td>1.557973</td>\n",
       "      <td>2.056682</td>\n",
       "      <td>547.943392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>231.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>414.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>737.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>91.00000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>4998.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              stars       useful         funny          cool        length\n",
       "count  10000.000000  10000.00000  10000.000000  10000.000000  10000.000000\n",
       "mean       3.716700      1.29820      0.458000      0.559000    581.699600\n",
       "std        1.471549      2.90264      1.557973      2.056682    547.943392\n",
       "min        1.000000      0.00000      0.000000      0.000000      2.000000\n",
       "25%        3.000000      0.00000      0.000000      0.000000    231.000000\n",
       "50%        4.000000      0.00000      0.000000      0.000000    414.000000\n",
       "75%        5.000000      1.00000      0.000000      0.000000    737.000000\n",
       "max        5.000000     91.00000     42.000000     86.000000   4998.000000"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>stars</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WOW!! This company is amazing!!!!! As a full t...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We had an appointment for 6:50 and we are stil...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Review is purely on the food because this is j...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>First of all, the inside is super clean and or...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I had seen this place featured on some Charlot...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I stopped because it was very early (before 8a...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I had a meeting for an estimate on June 1. Chr...</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>A fantastic place to train, especially if you ...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I come here frequently for an upper lip wax. T...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I have been going to this nail salon for years...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>As a non hot dog person I was not super excite...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>A great choice if you want something warm, fas...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>First off, Bookmans is definitely better than ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Regardless of the rental car company, you need...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Today was my first time eating at Served.   I ...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>So nice to see the old (Tatum &amp; Shea) Bombay s...</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>We went for dinner.  There is something to sui...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Sunday 6/24/2018 Jose came out to assist us wi...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Waited 2 hours as expected since this is still...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>They're located inside the Partell Pharmacy so...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>This Blimpie location is the best I've experie...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>I never realized this Khoury's was tucked behi...</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>I've been there about 15 times by now....they ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>This was our fave pizza place. The monster piz...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>I have nothing but good things to say about Ad...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Broke my power button while replacing my broke...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Fairly new establishment as I'm told. Plenty o...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>I had ordered flowers a day in advance to be d...</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Great atmosphere, there sampler plate is fried...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>I've been going to this Snap Fitness location ...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2470</th>\n",
       "      <td>My lovely daughter Whitney wanted to take me t...</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>2223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2471</th>\n",
       "      <td>I never been to the \"old\" Goose before the ren...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2472</th>\n",
       "      <td>Our server Haylee was fantastic! We came here ...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2473</th>\n",
       "      <td>I went when this first opened had a free Coupo...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2474</th>\n",
       "      <td>The mango roll was very good!! This cozy place...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2475</th>\n",
       "      <td>My wife was in need of a new car after giving ...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2476</th>\n",
       "      <td>I came here originally in August because my br...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2477</th>\n",
       "      <td>Sunny was so friendly and the food was beyond ...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2478</th>\n",
       "      <td>Horrible music \\n\\nTypical Im gonna stare you ...</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2479</th>\n",
       "      <td>I reviewed this restaurant before and gave it ...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2480</th>\n",
       "      <td>Probably the best food places at the beaches. ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2481</th>\n",
       "      <td>$10 admission for kids tournaments and the ACs...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2482</th>\n",
       "      <td>This is one place in Vegas that's does ribs an...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2483</th>\n",
       "      <td>In Vegas for a youth basketball tournament and...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2484</th>\n",
       "      <td>Today I celebrated my birthday by a surprise l...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2485</th>\n",
       "      <td>Most Fun and Relaxing Surgery I have ever expe...</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2486</th>\n",
       "      <td>Every time I come to Vegas, I immediately book...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2487</th>\n",
       "      <td>First time here while on a family vacation. Gr...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2488</th>\n",
       "      <td>Second visit way better than the first when th...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2489</th>\n",
       "      <td>Lets just say i do indeed have a love hate rel...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2490</th>\n",
       "      <td>Hands down the best pizza I've ever had. The i...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2491</th>\n",
       "      <td>My friend and I went to Landmark for dessert o...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2492</th>\n",
       "      <td>My boyfriend and I decided to try a new place ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2493</th>\n",
       "      <td>First off when you walk through the doors.. th...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2494</th>\n",
       "      <td>This is possibly the cutest stop on Baldwin Vi...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2495</th>\n",
       "      <td>Fire Grilled Filet Mignon with White Cheddar M...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2496</th>\n",
       "      <td>We stayed in the Rosemary building, it was sup...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2497</th>\n",
       "      <td>I recently had my hair cut and balyaged by Nik...</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2498</th>\n",
       "      <td>Haven't been to Denny's in years bc I always h...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499</th>\n",
       "      <td>We all know how hard it is to find a reliable,...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>511</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2500 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  stars  useful  funny  \\\n",
       "0     WOW!! This company is amazing!!!!! As a full t...      5       0      0   \n",
       "1     We had an appointment for 6:50 and we are stil...      1       0      0   \n",
       "2     Review is purely on the food because this is j...      4       0      0   \n",
       "3     First of all, the inside is super clean and or...      5       1      0   \n",
       "4     I had seen this place featured on some Charlot...      5       1      0   \n",
       "5     I stopped because it was very early (before 8a...      1       2      0   \n",
       "6     I had a meeting for an estimate on June 1. Chr...      1      21      0   \n",
       "7     A fantastic place to train, especially if you ...      5       0      0   \n",
       "8     I come here frequently for an upper lip wax. T...      4       2      1   \n",
       "9     I have been going to this nail salon for years...      2       4      0   \n",
       "10    As a non hot dog person I was not super excite...      3       0      0   \n",
       "11    A great choice if you want something warm, fas...      5       0      0   \n",
       "12    First off, Bookmans is definitely better than ...      3       0      0   \n",
       "13    Regardless of the rental car company, you need...      3       0      0   \n",
       "14    Today was my first time eating at Served.   I ...      5       0      0   \n",
       "15    So nice to see the old (Tatum & Shea) Bombay s...      4       9      0   \n",
       "16    We went for dinner.  There is something to sui...      3       0      0   \n",
       "17    Sunday 6/24/2018 Jose came out to assist us wi...      5       0      0   \n",
       "18    Waited 2 hours as expected since this is still...      2       1      1   \n",
       "19    They're located inside the Partell Pharmacy so...      4       1      0   \n",
       "20    This Blimpie location is the best I've experie...      5       0      0   \n",
       "21    I never realized this Khoury's was tucked behi...      5       2      0   \n",
       "22    I've been there about 15 times by now....they ...      5       1      0   \n",
       "23    This was our fave pizza place. The monster piz...      1       1      0   \n",
       "24    I have nothing but good things to say about Ad...      5       0      0   \n",
       "25    Broke my power button while replacing my broke...      2       1      1   \n",
       "26    Fairly new establishment as I'm told. Plenty o...      5       3      0   \n",
       "27    I had ordered flowers a day in advance to be d...      1       7      3   \n",
       "28    Great atmosphere, there sampler plate is fried...      4       0      0   \n",
       "29    I've been going to this Snap Fitness location ...      4       0      0   \n",
       "...                                                 ...    ...     ...    ...   \n",
       "2470  My lovely daughter Whitney wanted to take me t...      3       7      4   \n",
       "2471  I never been to the \"old\" Goose before the ren...      3       1      1   \n",
       "2472  Our server Haylee was fantastic! We came here ...      5       0      0   \n",
       "2473  I went when this first opened had a free Coupo...      1       0      0   \n",
       "2474  The mango roll was very good!! This cozy place...      4       3      0   \n",
       "2475  My wife was in need of a new car after giving ...      5       0      0   \n",
       "2476  I came here originally in August because my br...      2       0      2   \n",
       "2477  Sunny was so friendly and the food was beyond ...      5       0      0   \n",
       "2478  Horrible music \\n\\nTypical Im gonna stare you ...      3       9      4   \n",
       "2479  I reviewed this restaurant before and gave it ...      5       0      0   \n",
       "2480  Probably the best food places at the beaches. ...      3       0      0   \n",
       "2481  $10 admission for kids tournaments and the ACs...      1       1      1   \n",
       "2482  This is one place in Vegas that's does ribs an...      4       0      0   \n",
       "2483  In Vegas for a youth basketball tournament and...      1       0      0   \n",
       "2484  Today I celebrated my birthday by a surprise l...      5       0      0   \n",
       "2485  Most Fun and Relaxing Surgery I have ever expe...      5      11      0   \n",
       "2486  Every time I come to Vegas, I immediately book...      5       1      0   \n",
       "2487  First time here while on a family vacation. Gr...      5       0      0   \n",
       "2488  Second visit way better than the first when th...      3       1      1   \n",
       "2489  Lets just say i do indeed have a love hate rel...      3       0      0   \n",
       "2490  Hands down the best pizza I've ever had. The i...      5       0      0   \n",
       "2491  My friend and I went to Landmark for dessert o...      3       0      0   \n",
       "2492  My boyfriend and I decided to try a new place ...      3       0      0   \n",
       "2493  First off when you walk through the doors.. th...      2       0      0   \n",
       "2494  This is possibly the cutest stop on Baldwin Vi...      3       3      2   \n",
       "2495  Fire Grilled Filet Mignon with White Cheddar M...      4       1      0   \n",
       "2496  We stayed in the Rosemary building, it was sup...      3       0      0   \n",
       "2497  I recently had my hair cut and balyaged by Nik...      5       2      0   \n",
       "2498  Haven't been to Denny's in years bc I always h...      5       0      0   \n",
       "2499  We all know how hard it is to find a reliable,...      5       0      0   \n",
       "\n",
       "      cool  length  \n",
       "0        0     564  \n",
       "1        0     152  \n",
       "2        0     634  \n",
       "3        0     388  \n",
       "4        1    1476  \n",
       "5        1     625  \n",
       "6        0     252  \n",
       "7        0     241  \n",
       "8        2     204  \n",
       "9        0    1247  \n",
       "10       0     975  \n",
       "11       0     888  \n",
       "12       0    1353  \n",
       "13       0     479  \n",
       "14       0     401  \n",
       "15       3    1457  \n",
       "16       0     339  \n",
       "17       0     843  \n",
       "18       0     901  \n",
       "19       0     460  \n",
       "20       0     362  \n",
       "21       0     544  \n",
       "22       0     190  \n",
       "23       0     833  \n",
       "24       0     774  \n",
       "25       1     118  \n",
       "26       0     504  \n",
       "27       0    1568  \n",
       "28       0     423  \n",
       "29       0     890  \n",
       "...    ...     ...  \n",
       "2470     7    2223  \n",
       "2471     0    1028  \n",
       "2472     0     140  \n",
       "2473     0     494  \n",
       "2474     3     156  \n",
       "2475     0    1031  \n",
       "2476     0    1557  \n",
       "2477     0     158  \n",
       "2478     7      99  \n",
       "2479     0     267  \n",
       "2480     0     970  \n",
       "2481     0     337  \n",
       "2482     1     194  \n",
       "2483     0     651  \n",
       "2484     0     184  \n",
       "2485     0    1047  \n",
       "2486     1     443  \n",
       "2487     0     715  \n",
       "2488     1    1082  \n",
       "2489     0     298  \n",
       "2490     0     338  \n",
       "2491     0     214  \n",
       "2492     0     240  \n",
       "2493     0    1016  \n",
       "2494     6    1016  \n",
       "2495     0     164  \n",
       "2496     0     669  \n",
       "2497     1     489  \n",
       "2498     0     462  \n",
       "2499     0     511  \n",
       "\n",
       "[2500 rows x 6 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['cleaned_text'] = df_train['text'].apply(clean)\n",
    "df_test['cleaned_text'] = df_test['text'].apply(clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving all the necessary data in arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df_train['cleaned_text'].values\n",
    "test = df_test['cleaned_text'].values\n",
    "trainStars = df_train['stars'].values\n",
    "testStars = df_test['stars'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOW model with Naïve Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sequence item 0: expected str instance, list found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-bbed17505edc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrainVecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mtestVecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: sequence item 0: expected str instance, list found"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "trainVecs = vectorizer.fit_transform(train)\n",
    "testVecs = vectorizer.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifierNB = MultinomialNB()\n",
    "classifierNB.fit(trainVecs, trainStars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testStars_predicted = classifierNB.predict(testVecs)\n",
    "print(classifierNB.score(testVecs, testStars)*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(testStars, testStars_predicted, labels=None, sample_weight=None)\n",
    "\n",
    "df_cm = pd.DataFrame(confusion_matrix, index = [i for i in \"12345\"],\n",
    "                  columns = [i for i in \"12345\"])\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.heatmap(df_cm, annot=True, fmt='g')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_percentage = confusion_matrix / df_train.shape[0] * 100\n",
    "\n",
    "df_cm = pd.DataFrame(confusion_matrix_percentage, index = [i for i in \"12345\"],\n",
    "                  columns = [i for i in \"12345\"])\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.heatmap(df_cm, annot=True, fmt='g')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_proportions = []\n",
    "for n in range(confusion_matrix.shape[0]):\n",
    "    confusion_matrix_proportions.append(confusion_matrix[n,:]/df_test.groupby('stars').count().at[n+1, 'text']*100)\n",
    "    \n",
    "confusion_matrix_proportions = num.array(confusion_matrix_proportions)\n",
    "\n",
    "df_cm = pd.DataFrame(confusion_matrix_proportions, index = [i for i in \"12345\"],\n",
    "                  columns = [i for i in \"12345\"])\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.heatmap(df_cm, annot=True, fmt='g')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "distance = num.abs(testStars - testStars_predicted)\n",
    "collections.Counter(distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "train_tokenized = [word_tokenize(token) for token in train]\n",
    "test_tokenized = [word_tokenize(token) for token in test]\n",
    "\n",
    "word2vec = Word2Vec(train_tokenized)\n",
    "\n",
    "word2vec.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2vec(reviewTok):\n",
    "    vecSize = len(word2vec.wv[reviewTok[0][0]])\n",
    "    reviewVec = []\n",
    "    for sentence in reviewTok:\n",
    "        vectorSum = num.zeros(vecSize)\n",
    "        empty = True\n",
    "        for token in sentence:\n",
    "            if token in word2vec.wv.vocab:\n",
    "                vectorSum += word2vec.wv[token]\n",
    "                empty = False\n",
    "        if not empty:\n",
    "            vectorSum = vectorSum / num.sqrt((vectorSum ** 2).sum())\n",
    "        reviewVec.append(vectorSum)\n",
    "    return num.array(reviewVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors = sent2vec(train_tokenized)\n",
    "test_vectors = sent2vec(test_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xgbclassifier = XGBClassifier()\n",
    "xgbclassifier.fit(train_vectors, trainStars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xgbclassifier.score(test_vectors, testStars)*100, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking for relation between text length and rating\n",
    "Tests with charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(data=df_test, col='stars')\n",
    "g.map(plt.hist, 'length', bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='stars', y='length', data=df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starProp_train = df_train.groupby('stars').count().drop('useful', axis=1).drop('funny', axis=1).drop('cool', axis=1).drop('length', axis=1)\n",
    "starProp_train['percentage'] = starProp_train['cleaned_text'] / df_train.shape[0] * 100\n",
    "starProp_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starProp_test = df_test.groupby('stars').count().drop('useful', axis=1).drop('funny', axis=1).drop('cool', axis=1).drop('length', axis=1)\n",
    "starProp_test['percentage'] = starProp_test['cleaned_text'] / df_test.shape[0] * 100\n",
    "starProp_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and test set have same proportions! :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ideas:\n",
    "- make a numbers token <NUMBER>\n",
    "- stem the words\n",
    "- remove stopwords\n",
    "- create a personal noun token <PERSONALNOUN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "review = word_tokenize(df_test['cleaned_text'].values[4])\n",
    "stemmed = ''\n",
    "for word in review:\n",
    "    stemmed += stemmer.stem(word) + ' '\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
