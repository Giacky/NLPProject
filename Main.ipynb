{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as num\n",
    "import scipy as sci\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import sklearn as skl\n",
    "import xgboost\n",
    "import gensim\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingAmount = 100000\n",
    "testAmount = 25000\n",
    "\n",
    "df_train = pd.read_json('training' + str(trainingAmount) +'.json', lines=True)\n",
    "df_test = pd.read_json('test' + str(testAmount) +'.json', lines=True)\n",
    "\n",
    "df_test = df_test.drop(\"review_id\", axis=1).drop(\"business_id\", axis=1).drop(\"user_id\", axis=1).drop(\"date\", axis=1)\n",
    "df_test = df_test.reindex(['text','stars','useful','funny','cool'], axis=1)\n",
    "\n",
    "df_train = df_train.drop(\"review_id\", axis=1).drop(\"business_id\", axis=1).drop(\"user_id\", axis=1).drop(\"date\", axis=1)\n",
    "df_train = df_train.reindex(['text','stars','useful','funny','cool'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the clean method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    from nltk.stem.snowball import SnowballStemmer\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    \n",
    "     # Empty question\n",
    "    if type(text) != str:\n",
    "        return ''\n",
    "    \n",
    "    text = text.lower()\n",
    "    \n",
    "    # remove comma between numbers, i.e. 15,000 -> 15000\n",
    "    text = re.sub('(?<=[0-9])\\,(?=[0-9])', \"\", text)\n",
    "    \n",
    "    text = re.sub('[' + string.punctuation + ']', ' ', text)\n",
    "\n",
    "    text = re.sub('im', \"\", text)\n",
    "\n",
    "    \n",
    "    #Tokenizing text for normalizing more easily\n",
    "    tokenized_text = word_tokenize(text)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Removing stopwords\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    kept_words = ['he', 'she', 'they', 'we']\n",
    "    for kw in kept_words:\n",
    "        stopwords.remove(kw)\n",
    "    \n",
    "    tokenized_text = [w for w in tokenized_text if not w in stopwords] \n",
    "    \n",
    "    \n",
    "    #Removing numbers\n",
    "    tokenized_text = [w for w in tokenized_text if w.isalpha()]\n",
    "    \n",
    "    \n",
    "    #Putting tokens back together as a string\n",
    "    text = ''\n",
    "    for token in tokenized_text:\n",
    "        text += token + ' '\n",
    "        \n",
    "    \n",
    "    # Return a list of words\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['length'] = df_train['text'].apply(len)\n",
    "df_test['length'] = df_test['text'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stars</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.00000</td>\n",
       "      <td>100000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.736920</td>\n",
       "      <td>1.279980</td>\n",
       "      <td>0.442580</td>\n",
       "      <td>0.54412</td>\n",
       "      <td>586.933740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.454589</td>\n",
       "      <td>2.919489</td>\n",
       "      <td>1.648487</td>\n",
       "      <td>2.04579</td>\n",
       "      <td>548.694712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>232.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>416.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>746.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>94.00000</td>\n",
       "      <td>5000.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               stars         useful          funny          cool  \\\n",
       "count  100000.000000  100000.000000  100000.000000  100000.00000   \n",
       "mean        3.736920       1.279980       0.442580       0.54412   \n",
       "std         1.454589       2.919489       1.648487       2.04579   \n",
       "min         1.000000       0.000000       0.000000       0.00000   \n",
       "25%         3.000000       0.000000       0.000000       0.00000   \n",
       "50%         4.000000       0.000000       0.000000       0.00000   \n",
       "75%         5.000000       1.000000       0.000000       0.00000   \n",
       "max         5.000000     101.000000      74.000000      94.00000   \n",
       "\n",
       "              length  \n",
       "count  100000.000000  \n",
       "mean      586.933740  \n",
       "std       548.694712  \n",
       "min         1.000000  \n",
       "25%       232.000000  \n",
       "50%       416.000000  \n",
       "75%       746.000000  \n",
       "max      5000.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['cleaned_text'] = df_train['text'].apply(clean)\n",
    "df_test['cleaned_text'] = df_test['text'].apply(clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving all the necessary data in arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df_train['cleaned_text'].values\n",
    "test = df_test['cleaned_text'].values\n",
    "trainStars = df_train['stars'].values\n",
    "testStars = df_test['stars'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOW model with Na√Øve Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "trainVecs = vectorizer.fit_transform(train)\n",
    "testVecs = vectorizer.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifierNB = MultinomialNB()\n",
    "classifierNB.fit(trainVecs, trainStars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testStars_predicted = classifierNB.predict(testVecs)\n",
    "print(classifierNB.score(testVecs, testStars)*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(testStars, testStars_predicted, labels=None, sample_weight=None)\n",
    "\n",
    "df_cm = pd.DataFrame(confusion_matrix, index = [i for i in \"12345\"],\n",
    "                  columns = [i for i in \"12345\"])\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.heatmap(df_cm, annot=True, fmt='g')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_percentage = confusion_matrix / df_train.shape[0] * 100\n",
    "\n",
    "df_cm = pd.DataFrame(confusion_matrix_percentage, index = [i for i in \"12345\"],\n",
    "                  columns = [i for i in \"12345\"])\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.heatmap(df_cm, annot=True, fmt='g')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_proportions = []\n",
    "for n in range(confusion_matrix.shape[0]):\n",
    "    confusion_matrix_proportions.append(confusion_matrix[n,:]/df_test.groupby('stars').count().at[n+1, 'text']*100)\n",
    "    \n",
    "confusion_matrix_proportions = num.array(confusion_matrix_proportions)\n",
    "\n",
    "df_cm = pd.DataFrame(confusion_matrix_proportions, index = [i for i in \"12345\"],\n",
    "                  columns = [i for i in \"12345\"])\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.heatmap(df_cm, annot=True, fmt='g')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "distance = num.abs(testStars - testStars_predicted)\n",
    "collections.Counter(distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "train_tokenized = [word_tokenize(token) for token in train]\n",
    "test_tokenized = [word_tokenize(token) for token in test]\n",
    "\n",
    "word2vec = Word2Vec(train_tokenized, min_count=10)\n",
    "\n",
    "word2vec.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2vec(reviewTok):\n",
    "    vecSize = len(word2vec.wv[reviewTok[0][0]])\n",
    "    reviewVec = []\n",
    "    for sentence in reviewTok:\n",
    "        vectorSum = num.zeros(vecSize)\n",
    "        empty = True\n",
    "        for token in sentence:\n",
    "            if token in word2vec.wv.vocab:\n",
    "                vectorSum += word2vec.wv[token]\n",
    "                empty = False\n",
    "        if not empty:\n",
    "            vectorSum = vectorSum / num.sqrt((vectorSum ** 2).sum())\n",
    "        reviewVec.append(vectorSum)\n",
    "    return num.array(reviewVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors = sent2vec(train_tokenized)\n",
    "test_vectors = sent2vec(test_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xgbclassifier = XGBClassifier()\n",
    "xgbclassifier.fit(train_vectors, trainStars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xgbclassifier.score(test_vectors, testStars)*100, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking for relation between text length and rating\n",
    "Tests with charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(data=df_test, col='stars')\n",
    "g.map(plt.hist, 'length', bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='stars', y='length', data=df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starProp_train = df_train.groupby('stars').count().drop('useful', axis=1).drop('funny', axis=1).drop('cool', axis=1).drop('length', axis=1)\n",
    "starProp_train['percentage'] = starProp_train['cleaned_text'] / df_train.shape[0] * 100\n",
    "starProp_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starProp_test = df_test.groupby('stars').count().drop('useful', axis=1).drop('funny', axis=1).drop('cool', axis=1).drop('length', axis=1)\n",
    "starProp_test['percentage'] = starProp_test['cleaned_text'] / df_test.shape[0] * 100\n",
    "starProp_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and test set have same proportions! :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ideas:\n",
    "- make a numbers token <NUMBER>\n",
    "- stem the words\n",
    "- remove stopwords\n",
    "- create a personal noun token <PERSONALNOUN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
